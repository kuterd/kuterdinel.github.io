<!DOCTYPE html>
<html lang='en'>
<head>
    <meta name="author" content="Kuter Dinel">
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <style>
    </style>
    <link rel="stylesheet" href="highlight.css">
    <link rel="stylesheet" href="style.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,400;0,700;1,400;1,700&display=swap" rel="stylesheet">
    <meta charset="UTF-8">
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-7BCXXHYNCD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-7BCXXHYNCD');
    </script>
    <title>Online Softmax for dummies â€¢ Kuter Dinel's blog</title>

    <meta name="description" content="Simple explanation of online softmax computation.">
    <meta property="og:type" content="article" />
	<meta property="og:title" content="Online Softmax for dummies" />
    <meta property="og:description" content="Simple explanation of online softmax computation." />
    <meta name="twitter:creator" content="@KuterDinel" />
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>   
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

</head>
<body>
    <div id='center'>
        <div id="header">
                <h2 style="display:inline;margin:0"><a href="index.html" style="color:white">Kuter Dinel's blog</a></h2>
    <a href="index.html" class="highlight" id="back-button">&crarr; Go back</a>
        </div>
        <main id="center-content">
    <article>
    <header>
    <h1> Online Softmax for dummies</h1>
    <i style="float:right;">on <strong>23.01.2025</strong> by <strong>Kuter Dinel</strong>.</i></br>
    </header>
    <p>
    <p>Softmax is defined as:
$$softmax(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}$$</p>
<p>But since $e^{x_j}$ or $e^{x_i}$ can get very big, we can get an overflow or loose precision due to how floats work.</p>
<p>So most implementations use something called safe softmax, which gives the same result but doesn't have the problem of floats going ðŸ’¥</p>
<p>It works by subtracting the maximum x value $\max_{k = 1}^{K} x_k$ from $x_i$ and $x_j$ to get more stable calculations.</p>
<p>$$softmax(x_i) = \frac{e^{x_i - \max_{k = 1}^{K} x_k}}{\sum_{j=1}^{K} e^{x_j - \max_{k = 1}^{K} x_k}}$$</p>
<p>This gives the same result as regular sofmax, remember that $e^{a + b} = e^a e^b$ and $e^{a - b} = \frac{e^a}{e^b}$ so </p>
<p>$$\frac{e^{x_i - c}}{\sum_{j=1}^{K} e^{x_j - c}} = \frac{e^{x_i - c} / e^c}{(\sum_{j=1}^{K} e^{x_j - c}) / e^c} = \frac{e^{x_i}}{\sum_{j=1}^{K} e^{x_j}}$$</p>
<p>One problem with the safe softmax is that it takes three iterations over the data to produce the result. Once to find the maximum value, once to calculate the sum of exponents and once again for the final calculation.</p>
<p><a href="http://arxiv.org/abs/1805.02867">Online normalizer calculation for softmax</a>, this paper describes a technique to calculate the softmax in only two passes over the data.</p>
<p><img src="/images/online-softmax.png" width="700px" alt="Online softmax algorithm."></p>
<p>Paper gives a proof by induction on why the algorithm calculates softmax. </p>
<p>Here is the intuitive explanation:</p>
<p>Assume that $$d_{j - 1} = e^{x_0 - m_{j - 1}} + e^{x_1 - m_{j - 1}} + \ldots + e^{x_{j - 1} - m_{j - 1}}$$
Multiplying $d_{j - 1}$ with $e^{m_{j - 1} - m_j}$  $$d_{j - 1} e^{m_{j - 1} - m_j} = e^{x_0 - m_{j}} + e^{x_1 - m_{j}} + \ldots  + e^{x_{j - 1} - m_{j}}$$</p>
<p>So at the end of the loop </p>
<p>$$dV = \sum_{j=1}^{V} e^{x_j - \max_{k = 1}^{V}} x_k$$</p>
    </p>
    </article>
        </main>
        <footer id='footer'>
                <span>Kuter Dinel 2025 | Last Updated: 17-02-2025 | Blog Commit Hash: f83ffd1 </span>
        </footer>
    </div>
</body>